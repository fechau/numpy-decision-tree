{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import Data\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Filepath\n",
    "DATA_DIR = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test data in Data object\n",
    "train_data_array = np.loadtxt(DATA_DIR + 'train.csv', delimiter=',', dtype=str)\n",
    "train_data = Data(data=train_data_array)\n",
    "\n",
    "test_data_array = np.loadtxt(DATA_DIR + 'test.csv', delimiter=',', dtype=str)\n",
    "test_data = Data(data=test_data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For binary classification get which label is more likely\n",
    "def get_common_label(data):\n",
    "    labels = data.get_column('label')\n",
    "    \n",
    "    first_label = labels[0]\n",
    "    other_label = None\n",
    "    \n",
    "    num_total = len(labels)\n",
    "    num_first = 0\n",
    "    \n",
    "    for label in labels:\n",
    "        if label == first_label:\n",
    "            num_first += 1\n",
    "        elif other_label is None:\n",
    "            other_label = label\n",
    "            \n",
    "    if (num_first/num_total) >= 0.5:\n",
    "        return first_label\n",
    "    else:\n",
    "        return other_label\n",
    "\n",
    "# Pass in the data you want the entropy computed for\n",
    "def entropy(value_data):\n",
    "    \n",
    "    entropy = 0\n",
    "    labels = value_data.get_column('label')\n",
    "    \n",
    "    num_edible = len(value_data.get_row_subset('label', 'e'))\n",
    "    num_total = len(labels)\n",
    "    \n",
    "    # Probabilties\n",
    "    prob_edible = num_edible/num_total\n",
    "    prob_posionous = (num_total-num_edible)/num_total\n",
    "    \n",
    "    probabilities = [prob_edible, prob_posionous]\n",
    "    \n",
    "    for i in probabilities:\n",
    "        if i == 0:\n",
    "            continue\n",
    "        entropy += (-i*math.log(i,2))\n",
    "        # print((-i*math.log(i,2)))\n",
    "\n",
    "    # print(\"Entropy: \" + str(entropy))\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# Compute the expected entropy across all values for a certain attribute\n",
    "def expected_entropy(data, attribute):\n",
    "    \n",
    "    # Loop through each value for this attribute\n",
    "    possible_values = data.get_attribute_possible_vals(attribute)\n",
    "    \n",
    "    total_num = len(data)\n",
    "    expected_entropy = 0\n",
    "    \n",
    "    for v in possible_values:\n",
    "        subset = data.get_row_subset(attribute, v)\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "        expected_entropy += (len(subset)/total_num) * entropy(subset)\n",
    "    \n",
    "    # print('Expected entropy for ' + attribute + ': ' + str(expected_entropy))\n",
    "    \n",
    "    return expected_entropy\n",
    "\n",
    "def decide_split(data, attributes):\n",
    "    \n",
    "    # Calculate entropy over entire set\n",
    "    label_entropy = entropy(data)\n",
    "    \n",
    "    info_gain_list = []\n",
    "    \n",
    "    highest_gain = 0\n",
    "    highest_gain_index = -1\n",
    "    \n",
    "    # For each attribute in attributes calculate information gain\n",
    "    for i,attribute in enumerate(attributes):\n",
    "        # Skip if attribute is the label\n",
    "        if attribute == 'label':\n",
    "            continue\n",
    "            \n",
    "        info_gain = label_entropy-expected_entropy(data, attribute)\n",
    "        info_gain_list.append(info_gain)\n",
    "        \n",
    "        # print(\"Information gain for\", attribute, \": \", info_gain)\n",
    "        \n",
    "        # If this is the new highest gain then update\n",
    "        if info_gain > highest_gain:\n",
    "            highest_gain = info_gain\n",
    "            highest_gain_index = i\n",
    "    \n",
    "    #print(\"Splitting on\", attributes[highest_gain_index])\n",
    "    \n",
    "    return attributes[highest_gain_index]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define node class that contains branches and attributes\n",
    "class Node:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.is_label = False\n",
    "        self.branches = {}\n",
    "        \n",
    "        # Default setting for label\n",
    "        self.label = ''\n",
    "    \n",
    "    # Attribute getter/setter\n",
    "    def set_attribute(self, attribute):\n",
    "        self.attribute = attribute\n",
    "        \n",
    "    def get_attribute(self):\n",
    "        return self.attribute\n",
    "        \n",
    "    # Branch getter/setter\n",
    "    def add_branch(self, branch):\n",
    "        self.branches[branch] = None\n",
    "    \n",
    "    def get_branches(self):\n",
    "        return self.branches\n",
    "    \n",
    "    # Is Label getter/setter\n",
    "    def is_label(self):\n",
    "        return self.is_label\n",
    "    \n",
    "    def set_is_label(self, boolean):\n",
    "        self.is_label = boolean\n",
    "    \n",
    "    # Label getter/setter\n",
    "    def set_label(self, label):\n",
    "        self.label = label\n",
    "    \n",
    "    def get_label(self, label):\n",
    "        return self.label\n",
    "    \n",
    "    # Set child\n",
    "    def add_child(self, branch, child):\n",
    "        self.branches[branch] = child\n",
    "    \n",
    "    # Get max depth of the tree\n",
    "    def get_depth(self):\n",
    "        \n",
    "        if self.is_label:\n",
    "            return 1\n",
    "        \n",
    "        depths = []\n",
    "        \n",
    "        for branch in list(self.branches.values()):\n",
    "            depths.append(branch.get_depth()+1)\n",
    "        \n",
    "        return max(depths)\n",
    "    \n",
    "    # Predict one specific example at this node\n",
    "    def predict_example(self, example, column_index_dict):\n",
    "        if self.is_label:\n",
    "            return self.label\n",
    "        else:\n",
    "            # Get this attribute value\n",
    "            value = example[column_index_dict[self.attribute]]\n",
    "\n",
    "            # TODO: Handle case where a certain value wasn't in training data\n",
    "            if not value in self.branches:\n",
    "                return self.branches['other'].predict_example(example, column_index_dict)\n",
    "            else:\n",
    "                return self.branches[value].predict_example(example, column_index_dict)\n",
    "    \n",
    "    # Predict all examples from a data object\n",
    "    def predict(self, data_object):\n",
    "        \n",
    "        data = data_object.raw_data\n",
    "        column_index_dict = data_object.column_index_dict\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for d in data:\n",
    "            predictions.append(self.predict_example(d, column_index_dict))\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def describe(self):\n",
    "        if self.is_label:\n",
    "            return self.label\n",
    "        else:\n",
    "            return self.attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree class that can train and predict on new data\n",
    "class DecisionTree:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "        \n",
    "    # Define ID3 algorithm to use in decision tree\n",
    "    def ID3(self, S, attributes, label, current_depth):\n",
    "\n",
    "        # if we are limiting max depth then limit it \n",
    "        if self.max_depth != -1:\n",
    "            # If at max depth then take majority here \n",
    "            if current_depth >= self.max_depth:\n",
    "                # Return majority label here\n",
    "                leaf = Node()\n",
    "\n",
    "                leaf.set_label(get_common_label(S))\n",
    "                leaf.set_is_label(True)\n",
    "\n",
    "                return leaf\n",
    "        \n",
    "        # Check if all values in S have the same label\n",
    "        same_label = True\n",
    "        first_label = S.get_column(label)[0]\n",
    "        for temp_label in S.get_column(label):\n",
    "            if temp_label != first_label:\n",
    "                same_label = False\n",
    "                break\n",
    "\n",
    "        # If all labels are the same then return single node tree with label\n",
    "        if same_label:\n",
    "            leaf = Node()\n",
    "\n",
    "            leaf.set_label(first_label)\n",
    "            leaf.set_is_label(True)\n",
    "\n",
    "            return leaf\n",
    "\n",
    "        # Otherwise create root node for this tree\n",
    "        root = Node()\n",
    "\n",
    "        # Use information gain to find best attribute to split on\n",
    "        attribute_to_split_on = decide_split(S, attributes)\n",
    "        \n",
    "        # Set node attribute value\n",
    "        root.set_attribute(attribute_to_split_on)\n",
    "\n",
    "        # Get each possible value for that attribute\n",
    "        possible_values = S.get_attribute_possible_vals(attribute_to_split_on)\n",
    "\n",
    "        for branch in possible_values:\n",
    "            # Add new branch corresponding to this value\n",
    "            root.add_branch(branch)\n",
    "\n",
    "            # Get subset of this attribute equal to this value\n",
    "            subset = S.get_row_subset(attribute_to_split_on, branch)\n",
    "\n",
    "            # Check if this subset is empty\n",
    "            if len(subset) == 0:\n",
    "                # Add leaf node with common value of label is s\n",
    "                leaf = Node()\n",
    "                leaf.set_is_label(True)\n",
    "\n",
    "                # Get max label\n",
    "                leaf.set_label(get_common_label(S))\n",
    "\n",
    "                # Add leaf node\n",
    "                root.add_child(branch, leaf)\n",
    "            else:\n",
    "                subset_attributes = attributes.copy()\n",
    "                subset_attributes.remove(attribute_to_split_on)\n",
    "                root.add_child(branch, self.ID3(subset, subset_attributes, label, current_depth+1))\n",
    "\n",
    "        # Add other branch for values not in training data and use majority label\n",
    "        root.add_branch('other')\n",
    "\n",
    "        # Add child to use if value isn't in values\n",
    "        leaf = Node()\n",
    "        leaf.set_is_label(True)\n",
    "        leaf.set_label(get_common_label(S))\n",
    "        root.add_child('other', leaf)     \n",
    "\n",
    "        # Finally return root node\n",
    "        return root\n",
    "        \n",
    "    def train(self, data, max_depth=-1):\n",
    "        # Get a set of all measured attributes\n",
    "        measured_attributes_set = list(train_data.column_index_dict.keys())\n",
    "        \n",
    "        # Set max depth\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        self.root = self.ID3(data, measured_attributes_set, 'label', 0)\n",
    "        \n",
    "    def predict(self, data):\n",
    "        return self.root.predict(data)\n",
    "        \n",
    "    def get_depth(self):\n",
    "        # Minus one to account for root node\n",
    "        return self.root.get_depth()-1\n",
    "    \n",
    "    def print_tree(self):\n",
    "        self.root.describe_tree()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree \n",
    "decision_tree = DecisionTree()\n",
    "\n",
    "decision_tree.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = decision_tree.predict(train_data)\n",
    "\n",
    "train_actual = train_data.get_column('label')\n",
    "\n",
    "train_total = len(train_actual)\n",
    "train_correct = (train_predictions == train_actual).sum()\n",
    "\n",
    "print('Predicting on train data:')\n",
    "print('Error:', (train_total-train_correct)/train_total)\n",
    "print('Accuracy:', train_correct/train_total)\n",
    "print('Correct Predictions:',train_correct, '/', train_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = decision_tree.predict(test_data)\n",
    "\n",
    "test_actual = test_data.get_column('label')\n",
    "\n",
    "test_total = len(test_actual)\n",
    "test_correct = (test_predictions == test_actual).sum()\n",
    "\n",
    "print('Predicting on test data:')\n",
    "print('Error:', (test_total-test_correct)/test_total)\n",
    "print('Accuracy:', test_correct/test_total)\n",
    "print('Correct Predictions:',test_correct, '/', test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Max depth:', decision_tree.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross-validation\n",
    "FOLD_DATA_PATH = 'data/CVfolds_new/'\n",
    "\n",
    "# Load in folds\n",
    "folds = []\n",
    "\n",
    "# Get header row\n",
    "header_row = train_data_array[0]\n",
    "# Reshape header row\n",
    "header_row.shape\n",
    "header_row = np.expand_dims(header_row, axis=0)\n",
    "\n",
    "# Skip over header rows and read in folds\n",
    "for i in range(5):\n",
    "    folds.append(np.loadtxt(FOLD_DATA_PATH + 'fold%s.csv' % (str(i+1)), delimiter=',', dtype=str, skiprows=1))\n",
    "\n",
    "# Different depths\n",
    "depths = [1,2,3,4,5,10,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over different depths\n",
    "depths_accuracy = []\n",
    "depths_std_dev = []\n",
    "\n",
    "for j, depth in enumerate(depths):\n",
    "    # Loop over different groups and calculate average accuracy and std dev\n",
    "    folds_accuracy = []\n",
    "\n",
    "    for i in range(len(folds)):\n",
    "        accuracy_list = []\n",
    "\n",
    "        # Remove validation fold for this iteration\n",
    "        folds_copy = folds.copy()\n",
    "        del folds_copy[i]\n",
    "\n",
    "        # Combine other folds\n",
    "        training_folds = np.concatenate((folds_copy))\n",
    "        # Add header row\n",
    "        training_folds = np.concatenate((header_row, training_folds))\n",
    "        \n",
    "        # Add header row to validation data\n",
    "        validation_fold = np.concatenate((header_row, folds[i]))\n",
    "        \n",
    "        # Use decision tree\n",
    "        fold_decision_tree = DecisionTree()\n",
    "        fold_decision_tree.train(Data(data=training_folds), max_depth=depth)\n",
    "        \n",
    "        fold_predictions = fold_decision_tree.predict(Data(data=validation_fold))\n",
    "        fold_correct = (fold_predictions == Data(data=validation_fold).get_column('label')).sum()\n",
    "        \n",
    "        folds_accuracy.append(fold_correct/len(fold_predictions))\n",
    "        \n",
    "    # Add depth average and std dev\n",
    "    depths_accuracy.append(np.average(folds_accuracy))\n",
    "    depths_std_dev.append(np.std(folds_accuracy))\n",
    "\n",
    "# Print out average accuracies and std dev\n",
    "for i in range(len(depths_accuracy)):\n",
    "    print('Depth', depths[i], 'average accuracy =', depths_accuracy[i], 'std dev =', depths_std_dev[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the tree with the new max depth chosen from the cross validation\n",
    "decision_tree = DecisionTree()\n",
    "decision_tree.train(train_data, max_depth=10)\n",
    "print('Training tree with max depth of 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = decision_tree.predict(test_data)\n",
    "\n",
    "test_actual = test_data.get_column('label')\n",
    "\n",
    "test_total = len(test_actual)\n",
    "test_correct = (test_predictions == test_actual).sum()\n",
    "\n",
    "print('Predicting on test data with depth of 10:')\n",
    "print('Error:', (test_total-test_correct)/test_total)\n",
    "print('Accuracy:', test_correct/test_total)\n",
    "print('Correct Predictions:',test_correct, '/', test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
